import { useState } from 'react';
import Layout from '../../components/Layout';
import Head from 'next/head';

export default function RobotsTxtGenerator() {
  const [config, setConfig] = useState({
    userAgent: '*',
    allow: [],
    disallow: ['/admin/', '/private/', '/temp/'],
    crawlDelay: '',
    sitemap: 'https://example.com/sitemap.xml'
  });
  const [customRule, setCustomRule] = useState('');
  const [generatedRobots, setGeneratedRobots] = useState('');

  const handleGenerate = () => {
    let robotsTxt = `# Generated by ProURLMonitor - AI Robots.txt Generator\n`;
    robotsTxt += `# ${new Date().toLocaleDateString()}\n\n`;
    robotsTxt += `User-agent: ${config.userAgent}\n`;
    
    if (config.allow.length > 0) {
      config.allow.forEach(path => {
        robotsTxt += `Allow: ${path}\n`;
      });
    }
    
    if (config.disallow.length > 0) {
      config.disallow.forEach(path => {
        robotsTxt += `Disallow: ${path}\n`;
      });
    }
    
    if (config.crawlDelay) {
      robotsTxt += `Crawl-delay: ${config.crawlDelay}\n`;
    }
    
    robotsTxt += `\n`;
    
    if (config.sitemap) {
      robotsTxt += `Sitemap: ${config.sitemap}\n`;
    }

    setGeneratedRobots(robotsTxt);
  };

  const addDisallow = () => {
    if (customRule && !config.disallow.includes(customRule)) {
      setConfig({...config, disallow: [...config.disallow, customRule]});
      setCustomRule('');
    }
  };

  const removeDisallow = (path) => {
    setConfig({...config, disallow: config.disallow.filter(p => p !== path)});
  };

  const copyToClipboard = () => {
    navigator.clipboard.writeText(generatedRobots);
    alert('Robots.txt copied to clipboard!');
  };

  const downloadFile = () => {
    const element = document.createElement('a');
    const file = new Blob([generatedRobots], {type: 'text/plain'});
    element.href = URL.createObjectURL(file);
    element.download = 'robots.txt';
    document.body.appendChild(element);
    element.click();
    document.body.removeChild(element);
  };

  return (
    <Layout>
      <Head>
        <title>AI Robots.txt Generator - Create SEO-Friendly Robots.txt | ProURLMonitor</title>
        <meta name="description" content="Free AI-powered robots.txt generator. Create SEO-friendly robots.txt files with user-agent rules, crawl delays, and sitemap URLs. Download instantly." />
        <meta name="keywords" content="robots.txt generator, robots txt creator, SEO robots file, user-agent rules, crawl delay, sitemap generator, search engine crawlers" />
        <meta property="og:title" content="AI Robots.txt Generator - ProURLMonitor" />
        <meta property="og:description" content="Create SEO-friendly robots.txt files with our AI-powered generator" />
        <meta property="og:url" content="https://prourlmonitor.com/tools/robots-txt-generator" />
        <link rel="canonical" content="https://prourlmonitor.com/tools/robots-txt-generator" />
        <meta name="robots" content="index, follow" />
      </Head>

      <div className="max-w-6xl mx-auto px-4 py-8">
        <div className="text-center mb-8">
          <h1 className="text-4xl font-bold text-emerald-800 mb-3">AI Robots.txt Generator</h1>
          <p className="text-gray-600 text-lg">Create SEO-friendly robots.txt files for your website</p>
        </div>

        <div className="grid grid-cols-1 lg:grid-cols-2 gap-6">
          {/* Configuration Panel */}
          <div className="bg-white rounded-lg shadow-md p-6">
            <h2 className="text-2xl font-bold text-emerald-700 mb-4">Configuration</h2>
            
            <div className="space-y-4">
              <div>
                <label className="block text-sm font-medium text-gray-700 mb-2">User-agent</label>
                <select
                  value={config.userAgent}
                  onChange={(e) => setConfig({...config, userAgent: e.target.value})}
                  className="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-emerald-500"
                >
                  <option value="*">* (All Bots)</option>
                  <option value="Googlebot">Googlebot</option>
                  <option value="Bingbot">Bingbot</option>
                  <option value="Slurp">Yahoo Slurp</option>
                  <option value="DuckDuckBot">DuckDuckBot</option>
                </select>
              </div>

              <div>
                <label className="block text-sm font-medium text-gray-700 mb-2">Disallow Paths</label>
                <div className="flex gap-2 mb-2">
                  <input
                    type="text"
                    value={customRule}
                    onChange={(e) => setCustomRule(e.target.value)}
                    placeholder="/private/"
                    className="flex-1 px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-emerald-500"
                  />
                  <button
                    onClick={addDisallow}
                    className="btn btn-primary px-4 py-2"
                  >
                    Add
                  </button>
                </div>
                <div className="space-y-2">
                  {config.disallow.map((path, index) => (
                    <div key={index} className="flex items-center justify-between bg-emerald-50 px-3 py-2 rounded">
                      <span className="text-sm text-gray-700">{path}</span>
                      <button
                        onClick={() => removeDisallow(path)}
                        className="text-red-600 hover:text-red-700"
                      >
                        âœ•
                      </button>
                    </div>
                  ))}
                </div>
              </div>

              <div>
                <label className="block text-sm font-medium text-gray-700 mb-2">Crawl Delay (seconds)</label>
                <input
                  type="number"
                  value={config.crawlDelay}
                  onChange={(e) => setConfig({...config, crawlDelay: e.target.value})}
                  placeholder="Optional"
                  className="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-emerald-500"
                />
              </div>

              <div>
                <label className="block text-sm font-medium text-gray-700 mb-2">Sitemap URL</label>
                <input
                  type="url"
                  value={config.sitemap}
                  onChange={(e) => setConfig({...config, sitemap: e.target.value})}
                  placeholder="https://example.com/sitemap.xml"
                  className="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-emerald-500"
                />
              </div>

              <button
                onClick={handleGenerate}
                className="w-full btn btn-primary py-3 text-lg"
              >
                Generate Robots.txt
              </button>
            </div>
          </div>

          {/* Output Panel */}
          <div className="bg-white rounded-lg shadow-md p-6">
            <h2 className="text-2xl font-bold text-emerald-700 mb-4">Generated Robots.txt</h2>
            
            {generatedRobots ? (
              <>
                <pre className="bg-gray-50 p-4 rounded-lg text-sm font-mono overflow-auto max-h-96 mb-4 border border-gray-200">
                  {generatedRobots}
                </pre>
                
                <div className="flex gap-3">
                  <button
                    onClick={copyToClipboard}
                    className="flex-1 btn btn-secondary py-2"
                  >
                    ðŸ“‹ Copy
                  </button>
                  <button
                    onClick={downloadFile}
                    className="flex-1 btn btn-primary py-2"
                  >
                    ðŸ’¾ Download
                  </button>
                </div>
              </>
            ) : (
              <div className="text-center text-gray-400 py-20">
                <p className="text-lg">Configure and generate your robots.txt</p>
              </div>
            )}
          </div>
        </div>

        {/* Info Section */}
        <div className="bg-emerald-50 rounded-lg p-6 mt-6">
          <h3 className="text-xl font-bold text-emerald-800 mb-3">What is Robots.txt?</h3>
          <p className="text-gray-700 mb-3">
            Robots.txt is a text file that tells search engine crawlers which pages or files they can or can't request from your website. It's used to manage crawler traffic and prevent overloading your server.
          </p>
          <ul className="list-disc list-inside space-y-2 text-gray-700">
            <li>Control which areas of your site search engines can crawl</li>
            <li>Prevent duplicate content issues</li>
            <li>Keep private pages out of search results</li>
            <li>Specify sitemap location for better indexing</li>
          </ul>
        </div>
      </div>
    </Layout>
  );
}
