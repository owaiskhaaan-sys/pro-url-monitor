import { useState } from 'react';
import Layout from '../../components/Layout';
import Head from 'next/head';

export default function RobotsTxtGenerator() {
  const [sitemap, setSitemap] = useState('');
  const [disallowPaths, setDisallowPaths] = useState('/admin/\n/api/\n/login\n/signup');
  const [crawlDelay, setCrawlDelay] = useState('');
  const [generatedRobots, setGeneratedRobots] = useState('');

  const generateRobots = () => {
    let robotsTxt = '# robots.txt generated by ProURLMonitor\n\n';
    robotsTxt += 'User-agent: *\n';
    robotsTxt += 'Allow: /\n\n';

    // Add disallow paths
    if (disallowPaths.trim()) {
      const paths = disallowPaths.split('\n').filter(p => p.trim());
      if (paths.length > 0) {
        robotsTxt += '# Disallowed paths\n';
        paths.forEach(path => {
          robotsTxt += `Disallow: ${path.trim()}\n`;
        });
        robotsTxt += '\n';
      }
    }

    // Add crawl delay (optional)
    if (crawlDelay && parseInt(crawlDelay) > 0) {
      robotsTxt += `# Crawl delay\n`;
      robotsTxt += `Crawl-delay: ${crawlDelay}\n\n`;
    }

    // Add sitemap
    if (sitemap.trim()) {
      robotsTxt += '# Sitemap location\n';
      robotsTxt += `Sitemap: ${sitemap.trim()}\n`;
    }

    setGeneratedRobots(robotsTxt);
  };

  const copyToClipboard = () => {
    navigator.clipboard.writeText(generatedRobots);
    alert('Copied to clipboard!');
  };

  const downloadRobots = () => {
    const blob = new Blob([generatedRobots], { type: 'text/plain' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = 'robots.txt';
    a.click();
  };

  return (
    <Layout>
      <Head>
        <title>Robots.txt Generator - Create SEO-Friendly Robots File | ProURLMonitor</title>
        <meta name="description" content="Free robots.txt generator tool. Create custom robots.txt files for your website with sitemap, disallow rules, and crawl delay settings." />
      </Head>

      <div className="max-w-4xl mx-auto px-4 py-8">
        <h1 className="text-3xl sm:text-4xl font-bold text-emerald-700 mb-4">Robots.txt Generator</h1>
        <p className="text-gray-600 mb-8">Create a custom robots.txt file for your website to control search engine crawling</p>

        <div className="grid md:grid-cols-2 gap-6">
          {/* Input Section */}
          <div className="space-y-6">
            <div className="card">
              <h2 className="text-xl font-bold text-emerald-700 mb-4">Configure Your Robots.txt</h2>

              <div className="space-y-4">
                {/* Sitemap URL */}
                <div>
                  <label className="block text-sm font-medium text-gray-700 mb-2">
                    Sitemap URL
                  </label>
                  <input
                    type="text"
                    value={sitemap}
                    onChange={(e) => setSitemap(e.target.value)}
                    placeholder="https://example.com/sitemap.xml"
                    className="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-emerald-500"
                  />
                  <p className="text-xs text-gray-500 mt-1">Full URL to your sitemap file</p>
                </div>

                {/* Disallow Paths */}
                <div>
                  <label className="block text-sm font-medium text-gray-700 mb-2">
                    Disallow Paths (one per line)
                  </label>
                  <textarea
                    value={disallowPaths}
                    onChange={(e) => setDisallowPaths(e.target.value)}
                    rows="6"
                    placeholder="/admin/&#10;/api/&#10;/login"
                    className="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-emerald-500 font-mono text-sm"
                  />
                  <p className="text-xs text-gray-500 mt-1">Paths that search engines should not crawl</p>
                </div>

                {/* Crawl Delay */}
                <div>
                  <label className="block text-sm font-medium text-gray-700 mb-2">
                    Crawl Delay (seconds) - Optional
                  </label>
                  <input
                    type="number"
                    value={crawlDelay}
                    onChange={(e) => setCrawlDelay(e.target.value)}
                    placeholder="0"
                    min="0"
                    className="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-emerald-500"
                  />
                  <p className="text-xs text-gray-500 mt-1">Note: Googlebot ignores this directive</p>
                </div>

                <button
                  onClick={generateRobots}
                  className="btn btn-primary w-full"
                >
                  ðŸ¤– Generate Robots.txt
                </button>
              </div>
            </div>
          </div>

          {/* Output Section */}
          <div className="space-y-6">
            <div className="card">
              <h2 className="text-xl font-bold text-emerald-700 mb-4">Generated Robots.txt</h2>

              {generatedRobots ? (
                <>
                  <div className="bg-gray-900 text-green-400 p-4 rounded-lg mb-4 font-mono text-sm overflow-x-auto">
                    <pre className="whitespace-pre-wrap">{generatedRobots}</pre>
                  </div>

                  <div className="flex gap-3">
                    <button
                      onClick={copyToClipboard}
                      className="btn btn-secondary flex-1"
                    >
                      ðŸ“‹ Copy
                    </button>
                    <button
                      onClick={downloadRobots}
                      className="btn btn-primary flex-1"
                    >
                      ðŸ’¾ Download
                    </button>
                  </div>
                </>
              ) : (
                <div className="text-center py-12 text-gray-500">
                  <p>Configure your settings and click "Generate Robots.txt"</p>
                </div>
              )}
            </div>

            {/* Quick Tips */}
            <div className="card bg-blue-50 border-blue-200">
              <h3 className="font-bold text-blue-700 mb-2">ðŸ’¡ Quick Tips</h3>
              <ul className="text-sm text-gray-700 space-y-1">
                <li>â€¢ Upload robots.txt to your website root directory</li>
                <li>â€¢ Always include your sitemap URL</li>
                <li>â€¢ Use /admin/ to block entire directories</li>
                <li>â€¢ Test with Google Search Console</li>
              </ul>
            </div>
          </div>
        </div>

        {/* SEO Content */}
        <div className="mt-12 card">
          <h2 className="text-2xl font-bold text-emerald-700 mb-4">What is Robots.txt and Why You Need It</h2>
          <div className="prose max-w-none text-gray-700 space-y-4">
            <p>
              A <strong>robots.txt file</strong> is a simple text file that tells search engine crawlers which pages or sections of your website they can and cannot access. Think of it as a bouncer at a club - it controls who gets in and what areas they can visit. Every website should have one, and creating it doesn't have to be complicated.
            </p>
            
            <h3 className="text-xl font-bold text-emerald-600 mt-6 mb-3">How Does Robots.txt Work?</h3>
            <p>
              When search engines like Google, Bing, or Yahoo visit your website, the first thing they look for is the <strong>robots.txt file</strong> at your root domain (like https://example.com/robots.txt). This file gives them instructions about what they should and shouldn't crawl. If certain pages are marked as "Disallow," well-behaved bots will skip those pages entirely.
            </p>
            <p>
              Here's the thing though - robots.txt is more like a suggestion than a law. Good bots (like Googlebot) respect it, but malicious bots might ignore it completely. So don't use robots.txt for security - it's purely for SEO and crawl budget management.
            </p>

            <h3 className="text-xl font-bold text-emerald-600 mt-6 mb-3">Common Use Cases for Robots.txt</h3>
            <p>
              You'll want to use a <strong>robots.txt generator</strong> to block certain areas of your site from being indexed. Typical examples include admin panels (/admin/), API endpoints (/api/), login pages, duplicate content, or staging environments. You might also want to prevent crawlers from accessing image or CSS files to save bandwidth, though this is less common nowadays.
            </p>
            <p>
              Another critical use is including your <strong>sitemap URL</strong> in robots.txt. This tells search engines exactly where to find the master list of all your important pages, making it easier for them to discover and index your content efficiently.
            </p>

            <h3 className="text-xl font-bold text-emerald-600 mt-6 mb-3">Best Practices for Creating Robots.txt</h3>
            <p>
              Keep it simple. Start with "User-agent: *" which targets all bots, then use "Allow: /" to permit crawling by default. Add specific "Disallow" directives only for paths you genuinely don't want indexed. Always include your sitemap URL at the bottom - this is crucial for SEO.
            </p>
            <p>
              Avoid blocking important resources like CSS and JavaScript files unless you have a specific reason. Google needs to see these to properly render and understand your pages. Also, don't use robots.txt to hide sensitive information - use proper authentication and password protection instead.
            </p>

            <h3 className="text-xl font-bold text-emerald-600 mt-6 mb-3">Testing Your Robots.txt File</h3>
            <p>
              After you create and upload your <strong>robots.txt file</strong>, test it using Google Search Console's robots.txt Tester tool. This shows you exactly how Googlebot interprets your directives and helps catch any mistakes before they impact your SEO. Common errors include accidentally blocking your entire site or using the wrong syntax.
            </p>

            <p className="text-lg font-semibold text-emerald-700 mt-6">
              Ready to create your robots.txt file? Use our free generator above to build a custom file in seconds. Then check your site's overall health with our <a href="/tools/seo-audit" className="text-emerald-600 hover:underline">SEO Audit tool</a> and verify all your URLs are working with the <a href="/tools/http-status-checker" className="text-emerald-600 hover:underline">HTTP Status Checker</a>.
            </p>
          </div>
        </div>
      </div>
    </Layout>
  );
}
