import { useState } from 'react';
import Layout from '../../components/Layout';
import Head from 'next/head';

export default function RobotsTxtGenerator() {
  const [sitemap, setSitemap] = useState('https://example.com/sitemap.xml');
  const [disallowPaths, setDisallowPaths] = useState('/admin/\n/api/\n/login\n/signup');
  const [crawlDelay, setCrawlDelay] = useState('');
  const [generatedRobots, setGeneratedRobots] = useState('');

  const generateRobots = () => {
    let robotsTxt = '# robots.txt generated by ProURLMonitor\n\n';
    robotsTxt += 'User-agent: *\n';
    robotsTxt += 'Allow: /\n\n';

    // Add disallow paths
    if (disallowPaths.trim()) {
      const paths = disallowPaths.split('\n').filter(p => p.trim());
      if (paths.length > 0) {
        robotsTxt += '# Disallowed paths\n';
        paths.forEach(path => {
          robotsTxt += `Disallow: ${path.trim()}\n`;
        });
        robotsTxt += '\n';
      }
    }

    // Add crawl delay (optional)
    if (crawlDelay && parseInt(crawlDelay) > 0) {
      robotsTxt += `# Crawl delay\n`;
      robotsTxt += `Crawl-delay: ${crawlDelay}\n\n`;
    }

    // Add sitemap
    if (sitemap.trim()) {
      robotsTxt += '# Sitemap location\n';
      robotsTxt += `Sitemap: ${sitemap.trim()}\n`;
    }

    setGeneratedRobots(robotsTxt);
  };

  const copyToClipboard = () => {
    navigator.clipboard.writeText(generatedRobots);
    alert('Copied to clipboard!');
  };

  const downloadRobots = () => {
    const blob = new Blob([generatedRobots], { type: 'text/plain' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = 'robots.txt';
    a.click();
  };

  return (
    <Layout>
      <Head>
        <title>Robots.txt Generator - Create SEO-Friendly | ProURLMonitor</title>
        <meta name="description" content="Free robots.txt generator tool. Create custom robots.txt files for your website with sitemap, disallow rules, and crawl delay settings." />
        <link rel="canonical" href="https://www.prourlmonitor.com/tools/robots-txt-generator" />
      </Head>

      <div className="max-w-4xl mx-auto px-4 py-8">
        <h1 className="text-3xl sm:text-4xl font-bold text-emerald-700 mb-4">Robots.txt Generator</h1>
        <p className="text-gray-600 mb-8">Create a custom robots.txt file for your website to control search engine crawling</p>

        <div className="grid md:grid-cols-2 gap-6">
          {/* Input Section */}
          <div className="space-y-6">
            <div className="card">
              <h2 className="text-xl font-bold text-emerald-700 mb-4">Configure Your Robots.txt</h2>

              <div className="space-y-4">
                {/* Sitemap URL */}
                <div>
                  <label className="block text-sm font-medium text-gray-700 mb-2">
                    Sitemap URL
                  </label>
                  <input
                    type="text"
                    value={sitemap}
                    onChange={(e) => setSitemap(e.target.value)}
                    placeholder="https://example.com/sitemap.xml"
                    className="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-emerald-500"
                  />
                  <p className="text-xs text-gray-500 mt-1">Full URL to your sitemap file</p>
                </div>

                {/* Disallow Paths */}
                <div>
                  <label className="block text-sm font-medium text-gray-700 mb-2">
                    Disallow Paths (one per line)
                  </label>
                  <textarea
                    value={disallowPaths}
                    onChange={(e) => setDisallowPaths(e.target.value)}
                    rows="6"
                    placeholder="/admin/&#10;/api/&#10;/login"
                    className="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-emerald-500 font-mono text-sm"
                  />
                  <p className="text-xs text-gray-500 mt-1">Paths that search engines should not crawl</p>
                </div>

                {/* Crawl Delay */}
                <div>
                  <label className="block text-sm font-medium text-gray-700 mb-2">
                    Crawl Delay (seconds) - Optional
                  </label>
                  <input
                    type="number"
                    value={crawlDelay}
                    onChange={(e) => setCrawlDelay(e.target.value)}
                    placeholder="0"
                    min="0"
                    className="w-full px-4 py-2 border border-gray-300 rounded-lg focus:ring-2 focus:ring-emerald-500"
                  />
                  <p className="text-xs text-gray-500 mt-1">Note: Googlebot ignores this directive</p>
                </div>

                <button
                  onClick={generateRobots}
                  className="btn btn-primary w-full"
                >
                  ðŸ¤– Generate Robots.txt
                </button>
              </div>
            </div>
          </div>

          {/* Output Section */}
          <div className="space-y-6">
            <div className="card">
              <h2 className="text-xl font-bold text-emerald-700 mb-4">Generated Robots.txt</h2>

              {generatedRobots ? (
                <>
                  <div className="bg-gray-900 text-green-400 p-4 rounded-lg mb-4 font-mono text-sm overflow-x-auto">
                    <pre className="whitespace-pre-wrap">{generatedRobots}</pre>
                  </div>

                  <div className="flex gap-3">
                    <button
                      onClick={copyToClipboard}
                      className="btn btn-secondary flex-1"
                    >
                      ðŸ“‹ Copy
                    </button>
                    <button
                      onClick={downloadRobots}
                      className="btn btn-primary flex-1"
                    >
                      ðŸ’¾ Download
                    </button>
                  </div>
                </>
              ) : (
                <div className="text-center py-12 text-gray-500">
                  <p>Configure your settings and click "Generate Robots.txt"</p>
                </div>
              )}
            </div>

            {/* Quick Tips */}
            <div className="card bg-blue-50 border-blue-200">
              <h3 className="font-bold text-blue-700 mb-2">ðŸ’¡ Quick Tips</h3>
              <ul className="text-sm text-gray-700 space-y-1">
                <li>â€¢ Upload robots.txt to your website root directory</li>
                <li>â€¢ Always include your sitemap URL</li>
                <li>â€¢ Use /admin/ to block entire directories</li>
                <li>â€¢ Test with Google Search Console</li>
              </ul>
            </div>
          </div>
        </div>

        {/* SEO Content */}
        <div className="mt-12 card">
          <h2 className="text-2xl font-bold text-emerald-700 mb-4">What is Robots.txt and Why You Need It</h2>
          <div className="prose max-w-none text-gray-700 space-y-4">
            <p>
              A <strong>robots.txt file</strong> is a simple text file that tells search engine crawlers which pages or sections of your website they can and cannot access. Think of it as a bouncer at a club - it controls who gets in and what areas they can visit. Every website should have one, and creating it doesn't have to be complicated.
            </p>
            
            <h3 className="text-xl font-bold text-emerald-600 mt-6 mb-3">How Does Robots.txt Work?</h3>
            <p>
              When search engines like Google, Bing, or Yahoo visit your website, the first thing they look for is the <strong>robots.txt file</strong> at your root domain (like https://example.com/robots.txt). This file gives them instructions about what they should and shouldn't crawl. If certain pages are marked as "Disallow," well-behaved bots will skip those pages entirely.
            </p>
            <p>
              Here's the thing though - robots.txt is more like a suggestion than a law. Good bots (like Googlebot) respect it, but malicious bots might ignore it completely. So don't use robots.txt for security - it's purely for SEO and crawl budget management.
            </p>

            <h3 className="text-xl font-bold text-emerald-600 mt-6 mb-3">Common Use Cases for Robots.txt</h3>
            <p>
              You'll want to use a <strong>robots.txt generator</strong> to block certain areas of your site from being indexed. Typical examples include admin panels (/admin/), API endpoints (/api/), login pages, duplicate content, or staging environments. You might also want to prevent crawlers from accessing image or CSS files to save bandwidth, though this is less common nowadays.
            </p>
            <p>
              Another critical use is including your <strong>sitemap URL</strong> in robots.txt. This tells search engines exactly where to find the master list of all your important pages, making it easier for them to discover and index your content efficiently.
            </p>

            <h3 className="text-xl font-bold text-emerald-600 mt-6 mb-3">Best Practices for Creating Robots.txt</h3>
            <p>
              Keep it simple. Start with "User-agent: *" which targets all bots, then use "Allow: /" to permit crawling by default. Add specific "Disallow" directives only for paths you genuinely don't want indexed. Always include your sitemap URL at the bottom - this is crucial for SEO.
            </p>
            <p>
              Avoid blocking important resources like CSS and JavaScript files unless you have a specific reason. Google needs to see these to properly render and understand your pages. Also, don't use robots.txt to hide sensitive information - use proper authentication and password protection instead.
            </p>

            <h3 className="text-xl font-bold text-emerald-600 mt-6 mb-3">Testing Your Robots.txt File</h3>
            <p>
              After you create and upload your <strong>robots.txt file</strong>, test it using Google Search Console's robots.txt Tester tool. This shows you exactly how Googlebot interprets your directives and helps catch any mistakes before they impact your SEO. Common errors include accidentally blocking your entire site or using the wrong syntax.
            </p>

            <p className="text-lg font-semibold text-emerald-700 mt-6">
              Ready to create your robots.txt file? Use our free generator above to build a custom file in seconds. Then check your site's overall health with our <a href="/tools/seo-audit" className="text-emerald-600 hover:underline">SEO Audit tool</a> and verify all your URLs are working with the <a href="/tools/http-status-checker" className="text-emerald-600 hover:underline">HTTP Status Checker</a>.
            </p>
          </div>
        </div>
      </div>

        {/* Comprehensive Content Section */}
        <div className="max-w-4xl mx-auto px-4 py-8">
          <section className="prose prose-lg max-w-none">
            <h2 className="text-2xl font-bold text-emerald-800 mb-6">About This Tool</h2>
            <div className="text-gray-700 leading-relaxed space-y-4">
              <p className="mb-4">Our Robots.txt Generator is a powerful, free online tool designed to help you create customized robots.txt files for website crawling control. This professional-grade tool provides accurate results instantly, making it an essential resource for web developers, SEO professionals, digital marketers, and content creators. Whether you are working on a small personal project or managing enterprise-level campaigns, our tool delivers the reliability and precision you need.</p>
              <p className="mb-4">The Robots.txt Generator streamlines your workflow by automating complex tasks that would otherwise require manual effort or expensive software. With a user-friendly interface and instant results, you can complete your work faster and more efficiently. Our tool is completely web-based, meaning you do not need to install any software or plugins - simply open your browser and start using it immediately.</p>
              <p className="mb-4">Using a dedicated Robots.txt Generator offers numerous advantages over manual methods or generic solutions. First and foremost, it saves you valuable time by processing information quickly and accurately. Second, it eliminates human error that can occur when performing these tasks manually. Third, it provides consistent, standardized results that you can rely on for professional work.</p>
              <p className="mb-4">Professional users choose our Robots.txt Generator because it combines power with simplicity. You do not need technical expertise to use it effectively - the intuitive interface guides you through each step. At the same time, the tool offers advanced capabilities that satisfy the needs of experienced professionals who require precision and flexibility in their work.</p>
              <p className="mb-4">Our Robots.txt Generator includes features specifically designed for modern web workflows. The tool processes your requests instantly, providing results in real-time without delays. It handles both small-scale and large-scale operations efficiently, scaling to meet your specific needs. The clean, organized output format makes it easy to understand and use the results immediately.</p>
              <p className="mb-4">Security and privacy are paramount in our design. Your data is processed locally in your browser whenever possible, ensuring that sensitive information never leaves your device. For tools that require server processing, we use encrypted connections and never store your data permanently. You can use our tools with confidence, knowing that your information remains private and secure.</p>
              <p className="mb-4">The Robots.txt Generator serves multiple important use cases across different industries and professions. SEO specialists use it to control search engine crawling. Developers use it to protect sensitive pages. Webmasters use it to optimize crawl budget. Site owners use it to prevent duplicate content indexing. Each of these applications benefits from the speed, accuracy, and convenience that our tool provides.</p>
              <p className="mb-4">Web developers use this tool daily to streamline their development workflows and ensure code quality. SEO professionals rely on it for optimization tasks that improve search engine rankings. Content marketers leverage it to enhance their content strategy and measure performance. Digital agencies use it to deliver better results for their clients more efficiently.</p>
              <p className="mb-4">To get the most value from our Robots.txt Generator, follow these best practices. First, ensure you provide clean, well-formatted input data - while the tool can handle various formats, clean input produces the best results. Second, review the output carefully and understand what each metric or result means for your specific use case.</p>
              <p className="mb-4">For optimal results, integrate this tool into your regular workflow rather than using it as an occasional resource. Consistent use helps you become more familiar with its capabilities and identify patterns in your data. Many professionals bookmark this page and use it multiple times daily as part of their standard operating procedures.</p>
              <p className="mb-4">Consider combining this Robots.txt Generator with other tools in our suite for comprehensive analysis and optimization. Our tools are designed to work together, allowing you to build a complete workflow that addresses all aspects of your project. For example, you might use multiple tools in sequence to analyze, optimize, and validate your work.</p>
              <p className="mb-4">The Robots.txt Generator is built using modern web technologies that ensure fast performance and broad browser compatibility. It works seamlessly across all major browsers including Chrome, Firefox, Safari, and Edge. The responsive design adapts to any screen size, allowing you to use the tool effectively on desktop computers, laptops, tablets, and smartphones.</p>
              <p className="mb-4">We regularly update and improve the tool based on user feedback and evolving industry standards. Our development team monitors tool performance continuously and implements optimizations to maintain fast processing speeds. Updates are deployed automatically, so you always have access to the latest features and improvements without needing to download or install anything.</p>
              <p className="mb-4">Compared to alternative solutions, our Robots.txt Generator offers distinct advantages. Unlike software-based tools that require installation and updates, our web-based tool is always accessible and up-to-date. Unlike limited free tools that impose restrictions, our tool provides professional-grade capabilities without artificial limitations or usage caps.</p>
              <p className="mb-4">Many similar tools require account creation or subscription fees. We believe in providing value freely and openly, which is why our Robots.txt Generator is available to everyone without registration or payment. You can bookmark this page and return anytime you need the tool without worrying about subscriptions expiring or accounts being locked.</p>
              <p className="mb-4">If you encounter any issues or have questions about using the Robots.txt Generator, our support resources are here to help. The tool includes built-in help text and examples that guide you through the process. For more complex questions, you can refer to our comprehensive documentation or contact our support team who are happy to assist you.</p>
              <p className="mb-4">We welcome feedback and suggestions for improving the Robots.txt Generator. If you have ideas for new features or encounter bugs, please let us know so we can continue enhancing the tool. Your input directly influences our development priorities and helps us create tools that better serve the community.</p>
              <p className="mb-4">The Robots.txt Generator represents our commitment to providing high-quality, accessible tools for web professionals and enthusiasts. Whether you use it occasionally or rely on it daily, we hope it makes your work easier, faster, and more effective. Thank you for choosing our tool, and we look forward to supporting your continued success.</p>
            </div>
          </section>
        </div>

    </Layout>
  );
}
